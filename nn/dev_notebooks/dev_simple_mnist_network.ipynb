{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_d(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(activation, target):\n",
    "    return 0.5 * (target - activation) ** 2\n",
    "\n",
    "def cost_d(activation, target):\n",
    "    return (target - activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple ANN with 1 hidden layer\n",
    "\n",
    "Here we manually do backpropagation to learn weights for a simple nn with 30 hidden weights that will attempt to classify between 0 and 1 in the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28 * 28\n",
    "hidden_dim = 30 \n",
    "out_dim = 10\n",
    "\n",
    "hidden_weights = np.random.normal(0, 1, (hidden_dim, input_dim))\n",
    "out_weights = np.random.normal(0, 1, (out_dim, hidden_dim))\n",
    "\n",
    "hidden_biases = np.zeros((hidden_dim, 1))\n",
    "out_biases = np.zeros((out_dim, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    \"\"\"Compute a full forward pass of this small network\"\"\"\n",
    "    hidden_activations = sigmoid(np.dot(hidden_weights, x) + hidden_biases)\n",
    "    out_activations = sigmoid(np.dot(out_weights, hidden_activations) + out_biases)\n",
    "    out_activations /= np.sum(out_activations)\n",
    "    return out_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(x, y):\n",
    "    # Do a forward pass but cache the activations and z's \n",
    "    hidden_z = np.dot(hidden_weights, x) + hidden_biases\n",
    "    hidden_a = sigmoid(hidden_z)\n",
    "    out_z = np.dot(out_weights, hidden_a) + out_biases\n",
    "    out_a = sigmoid(out_z)\n",
    "#     out_a /= np.sum(out_a)\n",
    "    correct = np.argmax(out_a) == np.argmax(y)\n",
    "    # Now do the backwards pass\n",
    "    # Output layer\n",
    "    out_error = cost_d(out_a, y) * sigmoid_d(out_z)\n",
    "    # Partial derivatives\n",
    "    change_out_biases = out_error\n",
    "    change_out_weights = np.dot(out_error , hidden_a.T)\n",
    "    # Hidden layer\n",
    "    # Backpropagate the error\n",
    "    hidden_error = np.dot(out_weights.T, out_error) * sigmoid_d(hidden_z)\n",
    "    # Get the partial derivatives\n",
    "    change_hidden_biases = hidden_error\n",
    "    change_hidden_weights = np.dot(hidden_error, x.T)\n",
    "\n",
    "    return  correct, change_out_weights, change_out_biases, change_hidden_weights, change_hidden_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the MNIST dataset\n",
    "And just grab the 0 and 1 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loaders import MNISTLoader\n",
    "mloader = MNISTLoader()\n",
    "train_data, train_labels = mloader.get_training_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = train_labels.shape[0]\n",
    "\n",
    "# get images as vectors\n",
    "x = train_data.reshape(num_samples, -1).copy()\n",
    "# normalise the images\n",
    "mean, std = x.mean(1)[:, np.newaxis], x.std(1)[:, np.newaxis]\n",
    "x = x - mean\n",
    "x /= std\n",
    "x = x.reshape(num_samples, -1, 1)\n",
    "\n",
    "# one-hot encode labels\n",
    "y = np.zeros((num_samples, 10, 1))\n",
    "y[np.arange(num_samples),train_labels] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent\n",
    "\n",
    "Use SGD to train this small network for a few epochs and watch our training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5610 / 60000\n",
      "10077 / 60000\n",
      "12560 / 60000\n",
      "13725 / 60000\n",
      "14629 / 60000\n",
      "15335 / 60000\n",
      "15904 / 60000\n",
      "16361 / 60000\n",
      "16749 / 60000\n",
      "17087 / 60000\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "batch_size = 30 \n",
    "batch_starts = np.arange(batch_size, num_samples, batch_size)\n",
    "idxes = np.arange(0, num_samples, 1)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    np.random.shuffle(idxes)\n",
    "    batches = np.split(idxes, batch_starts)\n",
    "    total_correct = 0\n",
    "    for batch in batches:\n",
    "        # The total changes for the batch\n",
    "        delta_out_weights = np.zeros(out_weights.shape)\n",
    "        delta_out_biases = np.zeros(out_biases.shape)\n",
    "        delta_hidden_weights = np.zeros(hidden_weights.shape)\n",
    "        delta_hidden_biases = np.zeros(hidden_biases.shape)\n",
    "\n",
    "        for idx in batch:\n",
    "            correct, d_ow, d_ob, d_hw, d_hb = backpropagate(x[idx], y[idx])\n",
    "            total_correct += correct\n",
    "            delta_out_weights += d_ow\n",
    "            delta_out_biases += d_ob\n",
    "            delta_hidden_weights += d_hw\n",
    "            delta_hidden_biases += d_hb\n",
    "\n",
    "        out_weights += learning_rate * (1/batch_size) * delta_out_weights \n",
    "        out_biases += learning_rate * (1/batch_size) * delta_out_biases\n",
    "        hidden_weights += learning_rate * (1/batch_size) * delta_hidden_weights \n",
    "        hidden_biases += learning_rate * (1/batch_size) * delta_hidden_biases\n",
    "\n",
    "    print(f'{total_correct} / {num_samples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
