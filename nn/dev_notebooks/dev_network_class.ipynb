{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from activations import sigmoid, sigmoid_derivative\n",
    "from cost import MSE_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loaders import MNISTLoader\n",
    "mloader = MNISTLoader()\n",
    "train_data, train_labels = mloader.get_training_set()\n",
    "\n",
    "num_samples = train_labels.shape[0]\n",
    "# get images as vectors\n",
    "x = train_data.reshape(num_samples, -1)\n",
    "# one-hot encode labels\n",
    "y = np.zeros((num_samples, 10))\n",
    "y[np.arange(num_samples),train_labels] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_init_weights(size):\n",
    "    return np.random.normal(0, 1, size)\n",
    "\n",
    "class Network():\n",
    "    def __init__(self, layers=[]):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(1, len(layers)):\n",
    "            self.weights.append(random_init_weights((layers[i-1], layers[i])))\n",
    "            self.biases.append(np.zeros((1, layers[i])))\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        activations = batch\n",
    "        for weights, biases in zip(self.weights, self.biases):\n",
    "            activations = sigmoid(np.dot(activations, weights) + biases)\n",
    "        return activations\n",
    "    \n",
    "    def backward(self, batch, targets, dropout=0.5):\n",
    "        # Do the forward pass and cache the values\n",
    "        zs, activations = [], [batch]\n",
    "        for weights, biases in zip(self.weights, self.biases):\n",
    "            zs.append(np.dot(activations[-1], weights) + biases)\n",
    "            activation = sigmoid(zs[-1])\n",
    "            if dropout > 0 and len(activations) < len(self.weights):\n",
    "                dropout_mask = (np.random.rand(*activation.shape) < dropout) / dropout \n",
    "                activation *= dropout_mask\n",
    "            activations.append(activation)\n",
    "            \n",
    "        correct = np.count_nonzero(np.argmax(activations[-1], 1) == np.argmax(targets, 1)) \n",
    "        \n",
    "        # Backwards pass\n",
    "        weight_gradients = []\n",
    "        bias_gradients = []\n",
    "        \n",
    "        # The output layer error\n",
    "        error = MSE_derivative(activations[-1], targets) * sigmoid_derivative(zs[-1])\n",
    "        for l in range(2, len(self.weights)+2):\n",
    "            # Calculate the gradients from the error\n",
    "            bias_gradients.append(np.sum(error, 0))\n",
    "            weight_gradients.append(np.dot(activations[-l].T, error))\n",
    "            # Backpropagate the error until we get the input layer\n",
    "            if not l > len(zs):\n",
    "                error = np.dot(self.weights[-l+1], error.T).T * sigmoid_derivative(zs[-l])\n",
    "            \n",
    "        weight_gradients.reverse()\n",
    "        bias_gradients.reverse()\n",
    "        \n",
    "        return correct, weight_gradients, bias_gradients\n",
    "    \n",
    "    def fit(self, data, targets, epochs=10, batch_size=5, learning_rate=1e-3, reg_weight=1e-3)\n",
    "        num_samples = data.shape[0]\n",
    "        batch_starts = np.arange(batch_size, num_samples, batch_size)\n",
    "        idxes = np.arange(0, num_samples, 1)\n",
    "        for epoch in range(epochs):\n",
    "            np.random.shuffle(idxes)\n",
    "            batches = np.split(idxes, batch_starts)\n",
    "            total_correct = 0\n",
    "            for batch in batches:\n",
    "                c, weight_gradients, bias_gradients = self.backward(data[batch], targets[batch])\n",
    "                total_correct += c\n",
    "                for i in range(len(self.weights)):\n",
    "                    self.weights[i] = (1 - learning_rate * reg_weight / self.weights[i].size) * self.weights[i]  - learning_rate * (1/batch_size) * weight_gradients[i] \n",
    "                    self.biases[i] -= learning_rate * (1/batch_size) * bias_gradients[i] \n",
    "            print(f'Epoch {epoch} - {total_correct}/{num_samples}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28 * 28\n",
    "hidden_dim = 30 \n",
    "out_dim = 10\n",
    "\n",
    "net = Network([input_dim, 100, 30, out_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - 37149/60000\n",
      "Epoch 1 - 37650/60000\n",
      "Epoch 2 - 37921/60000\n",
      "Epoch 3 - 38290/60000\n",
      "Epoch 4 - 38283/60000\n",
      "Epoch 5 - 38375/60000\n",
      "Epoch 6 - 38832/60000\n",
      "Epoch 7 - 38930/60000\n",
      "Epoch 8 - 39129/60000\n",
      "Epoch 9 - 39370/60000\n",
      "Epoch 10 - 40199/60000\n",
      "Epoch 11 - 41202/60000\n",
      "Epoch 12 - 41402/60000\n",
      "Epoch 13 - 41621/60000\n",
      "Epoch 14 - 41816/60000\n",
      "Epoch 15 - 41835/60000\n",
      "Epoch 16 - 41957/60000\n",
      "Epoch 17 - 42139/60000\n",
      "Epoch 18 - 42175/60000\n",
      "Epoch 19 - 42116/60000\n",
      "Epoch 20 - 42256/60000\n",
      "Epoch 21 - 42217/60000\n",
      "Epoch 22 - 42367/60000\n",
      "Epoch 23 - 42174/60000\n",
      "Epoch 24 - 42090/60000\n",
      "Epoch 25 - 42150/60000\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "net.fit(x, y, epochs=50, batch_size=5, learning_rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - 52120/60000\n",
      "Epoch 1 - 52232/60000\n",
      "Epoch 2 - 52294/60000\n",
      "Epoch 3 - 52414/60000\n",
      "Epoch 4 - 53210/60000\n"
     ]
    }
   ],
   "source": [
    "net.fit(x, y, epochs=5, batch_size=5, learning_rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - 57800/60000\n",
      "Epoch 1 - 57845/60000\n",
      "Epoch 2 - 57913/60000\n",
      "Epoch 3 - 57945/60000\n",
      "Epoch 4 - 57992/60000\n"
     ]
    }
   ],
   "source": [
    "net.fit(x, y, epochs=5, batch_size=5, learning_rate=5e-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
