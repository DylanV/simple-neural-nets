{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_d(z):\n",
    "    return sigmoid(z) * (1.0 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(activation, target):\n",
    "    return 0.5 * (target - activation) ** 2\n",
    "\n",
    "def cost_d(activation, target):\n",
    "    return (target - activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple ANN with 1 hidden layer\n",
    "\n",
    "Here we manually do backpropagation to learn weights for a simple nn with 30 hidden weights that will attempt to classify between 0 and 1 in the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28 * 28\n",
    "hidden_dim = 30 \n",
    "out_dim = 2\n",
    "\n",
    "hidden_weights = np.random.normal(0, 1, (hidden_dim, input_dim))\n",
    "out_weights = np.random.normal(0, 1, (out_dim, hidden_dim))\n",
    "\n",
    "hidden_biases = np.zeros((hidden_dim, 1))\n",
    "out_biases = np.zeros((out_dim, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    \"\"\"Compute a full forward pass of this small network\"\"\"\n",
    "    hidden_activations = sigmoid(np.dot(hidden_weights, x) + hidden_biases)\n",
    "    out_activations = sigmoid(np.dot(out_weights, hidden_activations) + out_biases)\n",
    "    return out_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(x, y):\n",
    "    # Do a forward pass but cache the activations and z's \n",
    "    hidden_z = np.dot(hidden_weights, x) + hidden_biases\n",
    "    hidden_a = sigmoid(hidden_z)\n",
    "    out_z = np.dot(out_weights, hidden_a) + out_biases\n",
    "    out_a = sigmoid(out_z)\n",
    "    correct = np.argmax(out_a) == np.argmax(y)\n",
    "    # Now do the backwards pass\n",
    "    # Output layer\n",
    "    out_error = cost_d(out_a, y) * sigmoid_d(out_z)\n",
    "    # Partial derivatives\n",
    "    change_out_biases = out_error\n",
    "    change_out_weights = np.dot(out_error , hidden_a.T)\n",
    "    # Hidden layer\n",
    "    # Backpropagate the error\n",
    "    hidden_error = np.dot(out_weights.T, out_error) * sigmoid_d(hidden_z)\n",
    "    # Get the partial derivatives\n",
    "    change_hidden_biases = hidden_error\n",
    "    change_hidden_weights = np.dot(hidden_error, x.T)\n",
    "\n",
    "    return  correct, change_out_weights, change_out_biases, change_hidden_weights, change_hidden_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the MNIST dataset\n",
    "And just grab the 0 and 1 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loaders import MNISTLoader\n",
    "mloader = MNISTLoader()\n",
    "train_data, train_labels = mloader.get_training_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = np.logical_or(train_labels == 0,  train_labels == 1)\n",
    "train_labels = train_labels[keep]\n",
    "train_data = train_data[keep]\n",
    "num_samples = train_labels.shape[0]\n",
    "\n",
    "# get images as vectors\n",
    "x = train_data.reshape(num_samples, -1, 1)\n",
    "\n",
    "# one-hot encode labels\n",
    "y = np.zeros((num_samples, 2, 1))\n",
    "y[np.arange(num_samples),train_labels] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent\n",
    "\n",
    "Use SGD to train this small network for a few epochs and watch our training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7806 / 12665\n",
      "10909 / 12665\n",
      "12115 / 12665\n",
      "12330 / 12665\n",
      "12412 / 12665\n",
      "12450 / 12665\n",
      "12468 / 12665\n",
      "12482 / 12665\n",
      "12499 / 12665\n",
      "12511 / 12665\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 30 \n",
    "batch_starts = np.arange(batch_size, num_samples, batch_size)\n",
    "idxes = np.arange(0, num_samples, 1)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    np.random.shuffle(idxes)\n",
    "    batches = np.split(idxes, batch_starts)\n",
    "    total_correct = 0\n",
    "    for batch in batches:\n",
    "        # The total changes for the batch\n",
    "        delta_out_weights = np.zeros(out_weights.shape)\n",
    "        delta_out_biases = np.zeros(out_biases.shape)\n",
    "        delta_hidden_weights = np.zeros(hidden_weights.shape)\n",
    "        delta_hidden_biases = np.zeros(hidden_biases.shape)\n",
    "\n",
    "        for idx in batch:\n",
    "            correct, d_ow, d_ob, d_hw, d_hb = backpropagate(x[idx], y[idx])\n",
    "            total_correct += correct\n",
    "            delta_out_weights += d_ow\n",
    "            delta_out_biases += d_ob\n",
    "            delta_hidden_weights += d_hw\n",
    "            delta_hidden_biases += d_hb\n",
    "\n",
    "        out_weights += learning_rate * delta_out_weights \n",
    "        out_biases += learning_rate * delta_out_biases\n",
    "        hidden_weights += learning_rate * delta_hidden_weights \n",
    "        hidden_biases += learning_rate * delta_hidden_biases\n",
    "\n",
    "    print(f'{total_correct} / {num_samples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
